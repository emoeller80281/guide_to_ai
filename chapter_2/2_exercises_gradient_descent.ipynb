{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1179fa-80d6-4540-b246-419a435b5375",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1a087",
   "metadata": {},
   "source": [
    "## 2.18 Compute the loss landscape for a model other than Llama-3.2-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409551ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device='cuda'\n",
    "model = AutoModelForCausalLM.from_pretrained(\"crumb/nano-mistral\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"crumb/nano-mistral\")\n",
    "\n",
    "inputs = tokenizer([\"Once upon a time, there was a dog\"], return_tensors=\"pt\").to(model.device)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "#Tokenizer prepends a speical <|begin_of_text|> token\n",
    "for input_token_index in inputs['input_ids'].view(-1):\n",
    "    print(input_token_index.item(), tokenizer.decode(input_token_index))\n",
    "\n",
    "param_dict = {name: param for name, param in model.named_parameters()}\n",
    "print(\"\\nNum Layers\")\n",
    "print(len(param_dict)) #Our llama model contains 146 separate parameter tensors\n",
    "\n",
    "#Quick look at the sizes of the first tensors\n",
    "print(\"\\n\")\n",
    "for k, v in list(param_dict.items())[:16]:\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_directions(params, seed=None):\n",
    "    \"\"\"\n",
    "    Generate random direction vectors for each parameter tensor.\n",
    "    \n",
    "    Args:\n",
    "        params: List of (name, parameter) tuples from model.named_parameters()\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        direction: OrderedDict mapping parameter names to random direction tensors\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    direction = OrderedDict()\n",
    "    for name, param in params:\n",
    "        if param.requires_grad:\n",
    "            direction[name] = torch.randn_like(param.data)\n",
    "    \n",
    "    return direction\n",
    "\n",
    "def normalize_direction(direction, params):\n",
    "    \"\"\"\n",
    "    Normalize the direction tensors to match the norm of each parameter tensor.\n",
    "    \n",
    "    Args:\n",
    "        direction: OrderedDict mapping parameter names to direction tensors\n",
    "        params: List of (name, parameter) tuples from model.named_parameters()\n",
    "        \n",
    "    Returns:\n",
    "        normalized_direction: OrderedDict with normalized direction tensors\n",
    "    \"\"\"\n",
    "    param_dict = OrderedDict(params)\n",
    "    normalized_direction = OrderedDict()\n",
    "    \n",
    "    for name, dir_tensor in direction.items():\n",
    "        param_norm = torch.norm(param_dict[name].data)\n",
    "        dir_norm = torch.norm(dir_tensor)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if dir_norm > 0:\n",
    "            normalized_direction[name] = dir_tensor * (param_norm / dir_norm)\n",
    "        else:\n",
    "            normalized_direction[name] = dir_tensor\n",
    "    \n",
    "    return normalized_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_k=20,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "texts = tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n",
    "for t in texts:\n",
    "    print(t)\n",
    "\n",
    "prefix='pretrained_'\n",
    "filtered_params = [(name, p) for name, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "layers_name='first_4'\n",
    "filtered_params = filtered_params[1:37] #First 4 layers - I like this - favorite so far\n",
    "\n",
    "num_points=24 #Video uses 512, takes a few hours to compute\n",
    "random_seed_1 = 11\n",
    "random_seed_2 = 111\n",
    "\n",
    "# Generate and normalize two random directions\n",
    "direction1 = get_random_directions(filtered_params, seed=random_seed_1)\n",
    "direction2 = get_random_directions(filtered_params, seed=random_seed_2)\n",
    "\n",
    "direction1 = normalize_direction(direction1, filtered_params)\n",
    "direction2 = normalize_direction(direction2, filtered_params)\n",
    "\n",
    "# Store the original parameters in an ordered dict\n",
    "original_params = OrderedDict()\n",
    "for name, param in filtered_params:\n",
    "    original_params[name] = param.data.clone()\n",
    "\n",
    "# Create two linear spaces, one for each direction\n",
    "alphas=np.linspace(-2.5, 2.5, num_points)\n",
    "betas=np.linspace(-2.5, 2.5, num_points)\n",
    "\n",
    "losses=[]\n",
    "with torch.no_grad():\n",
    "    for i, alpha in enumerate(tqdm(alphas)):\n",
    "\n",
    "        losses.append([])\n",
    "        # Iterate through each alpha and beta value, computing the loss when we adjust\n",
    "        # the parameters by direction1 * alpha and direction2 * beta for the full grid\n",
    "        # of alpha and beta points.\n",
    "        for j, beta in enumerate(betas):\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in direction1:\n",
    "                    # Adjust all parameters using the current point in the grid\n",
    "                    param.data = original_params[name] + alpha * direction1[name] + beta*direction2[name]\n",
    "            \n",
    "            # Calculate the loss using the adjusted parameters\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "            # Calculate the loss only at position 5 in the input \"dog\", token 3914\n",
    "            dog_only_loss=-np.log(my_probs[0, 9, 3914].item())\n",
    "            losses[-1].append(dog_only_loss)\n",
    "    \n",
    "    for name, param in model.named_parameters(): # Restore original parameters\n",
    "        if name in original_params: \n",
    "            param.data.copy_(original_params[name])\n",
    "\n",
    "losses=np.array(losses)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contourf = ax.contourf(alphas, betas, losses, 20, cmap='viridis', alpha=0.8)\n",
    "contour = ax.contour(alphas, betas, losses, 30, colors='white', linewidths=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7977a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "Z = np.rot90(losses)             # or losses.T if that matches your axes\n",
    "A, B = np.meshgrid(alphas, betas)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "stride = max(1, len(alphas)//200)  # keep it responsive if grid is huge\n",
    "surf = ax.plot_surface(\n",
    "    A[::stride, ::stride],\n",
    "    B[::stride, ::stride],\n",
    "    Z[::stride, ::stride],\n",
    "    cmap=\"viridis\",\n",
    "    linewidth=0,\n",
    "    antialiased=True\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"beta\")\n",
    "ax.set_zlabel(\"Loss\")\n",
    "fig.colorbar(surf, ax=ax, shrink=0.6, pad=0.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install plotly\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "Z = np.rot90(losses / 2)  # shape (len(betas), len(alphas))\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(\n",
    "    x=alphas,\n",
    "    y=betas,\n",
    "    z=Z,\n",
    "    colorscale=\"Viridis\"\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"alpha\",\n",
    "        yaxis_title=\"beta\",\n",
    "        zaxis_title=\"Loss\",\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
